{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0cccee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1d4f70",
   "metadata": {},
   "source": [
    "### Count of images in each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72bed830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in train folder: 12601\n",
      "Number of images in test folder: 5410\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def count_images_in_folder(folder_path):\n",
    "    count = 0\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(('jpg', 'jpeg', 'png', 'bmp', 'gif')):\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "# Define the paths to the train and test folders\n",
    "train_folder_path = 'Human Action Recognition/train'\n",
    "test_folder_path = 'Human Action Recognition/test'\n",
    "\n",
    "# Count the images in each folder\n",
    "train_image_count = count_images_in_folder(train_folder_path)\n",
    "test_image_count = count_images_in_folder(test_folder_path)\n",
    "\n",
    "# Print the counts\n",
    "print(f'Number of images in train folder: {train_image_count}')\n",
    "print(f'Number of images in test folder: {test_image_count}')\n",
    "\n",
    "# Plotting the bar graph\n",
    "labels = ['Train', 'Test']\n",
    "counts = [train_image_count, test_image_count]\n",
    "\n",
    "plt.bar(labels, counts, color=['blue', 'orange'])\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.title('Number of Images in Train and Test Datasets')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630bb76a",
   "metadata": {},
   "source": [
    "### Copying the images and splitting based on class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f784f158",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files:  76%|██████████████████████████████████████████████▍              | 9585/12600 [01:01<00:18, 161.84it/s]"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "csv_file_path = 'Human Action Recognition/Training_set.csv'  # Update with your CSV file path\n",
    "base_directory = './Label-dataset/train'  # Update with your base directory path\n",
    "images_directory = 'Human Action Recognition/train'  # Update with your images directory path\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Get unique labels\n",
    "labels = df['label'].unique()\n",
    "\n",
    "# Create directories for each label\n",
    "for label in labels:\n",
    "    label_dir = os.path.join(base_directory, str(label))\n",
    "    os.makedirs(label_dir, exist_ok=True)\n",
    "\n",
    "# Copy files to corresponding directories with progress bar\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Copying files\"):\n",
    "    file_name = row['filename']\n",
    "    label = row['label']\n",
    "    \n",
    "    # Define the source and destination paths\n",
    "    src_path = os.path.join(images_directory, file_name)\n",
    "    dst_path = os.path.join(base_directory, str(label), file_name)\n",
    "    \n",
    "    # Copy the file\n",
    "    if os.path.exists(src_path):\n",
    "        shutil.copy(src_path, dst_path)\n",
    "    else:\n",
    "        print(f\"File {src_path} does not exist.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a980bf4",
   "metadata": {},
   "source": [
    "### Distribution of Classes in train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e827a73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = df['label'].value_counts()\n",
    "\n",
    "# Plot Bar Graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "label_counts.plot(kind='bar', color='skyblue')\n",
    "plt.title('Number of Images per Class')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Plot Pie Chart\n",
    "plt.figure(figsize=(8, 8))\n",
    "label_counts.plot(kind='pie', autopct='%1.1f%%', startangle=140, colors=plt.cm.Paired.colors)\n",
    "plt.title('Proportion of Images per Class')\n",
    "plt.ylabel('')  # Hides the 'y' label\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1004389",
   "metadata": {},
   "source": [
    "#### Data preprocessing: Data augumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c046379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "base_directory = './Label-dataset/train'  # The base directory where the images are stored\n",
    "\n",
    "# Create an instance of ImageDataGenerator with data augmentation and validation split\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,  # Rescale pixel values to [0, 1]\n",
    "    validation_split=0.2,  # Use 20% of the data for validation\n",
    ")\n",
    "\n",
    "# Create a data generator for the training set\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    base_directory,\n",
    "    target_size=(224, 224),  # Resize images to 256x256 pixels\n",
    "    batch_size=32,  # Number of images to be yielded from the generator per batch\n",
    "    color_mode='rgb',  # Images are in RGB format\n",
    "    class_mode='categorical',  # Use 'categorical' if you have multiple classes\n",
    "    subset='training'  # Set as training data\n",
    ")\n",
    "\n",
    "# Create a data generator for the validation set\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    base_directory,\n",
    "    target_size=(224, 224),  # Resize images to 256x256 pixels\n",
    "    batch_size=32,  # Number of images to be yielded from the generator per batch\n",
    "    color_mode='rgb',  # Images are in RGB format\n",
    "    class_mode='categorical',  # Use 'categorical' if you have multiple classes\n",
    "    subset='validation'  # Set as validation data\n",
    ")\n",
    "\n",
    "test_generator = datagen.flow_from_directory(\n",
    "    base_directory,\n",
    "    target_size=(224, 224),  # Resize images to 256x256 pixels\n",
    "    batch_size=32,  # Number of images to be yielded from the generator per batch\n",
    "    color_mode='rgb',  # Images are in RGB format\n",
    "    class_mode='categorical',  # Use 'categorical' if you have multiple classes\n",
    "    subset='validation'  # Set as validation data\n",
    ")\n",
    "\n",
    "# Example of how to iterate through the training generator and plot images\n",
    "images, labels = next(train_generator)\n",
    "\n",
    "# Plot a few images from the batch\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 20))\n",
    "axes = axes.flatten()\n",
    "for img, ax in zip(images[:5], axes):\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b86f100",
   "metadata": {},
   "source": [
    "### Data preprocessing: Implement Alphapose  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b4d0ba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# STEP 1: Import the necessary modules.\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "\n",
    "# Function to draw landmarks on image\n",
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "    pose_landmarks_list = detection_result.pose_landmarks\n",
    "    annotated_image = np.copy(rgb_image)\n",
    "\n",
    "    # Loop through the detected poses to visualize.\n",
    "    for idx in range(len(pose_landmarks_list)):\n",
    "        pose_landmarks = pose_landmarks_list[idx]\n",
    "\n",
    "        # Draw the pose landmarks.\n",
    "        pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "        pose_landmarks_proto.landmark.extend([\n",
    "            landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in pose_landmarks\n",
    "        ])\n",
    "        mp.solutions.drawing_utils.draw_landmarks(\n",
    "            annotated_image,\n",
    "            pose_landmarks_proto,\n",
    "            mp.solutions.pose.POSE_CONNECTIONS,\n",
    "            mp.solutions.drawing_styles.get_default_pose_landmarks_style())\n",
    "    return annotated_image\n",
    "\n",
    "# STEP 2: Create an PoseLandmarker object.\n",
    "base_options = python.BaseOptions(model_asset_path='pose_landmarker_heavy.task')\n",
    "options = vision.PoseLandmarkerOptions(\n",
    "    base_options=base_options,\n",
    "    output_segmentation_masks=True)\n",
    "detector = vision.PoseLandmarker.create_from_options(options)\n",
    "\n",
    "# STEP 3: Load the input image.\n",
    "image = mp.Image.create_from_file(\"Image_33.jpg\")\n",
    "\n",
    "# STEP 4: Detect pose landmarks from the input image.\n",
    "detection_result = detector.detect(image)\n",
    "\n",
    "# STEP 5: Process the detection result. In this case, visualize it.\n",
    "annotated_image = draw_landmarks_on_image(image.numpy_view(), detection_result)\n",
    "\n",
    "# Display the image using Matplotlib\n",
    "plt.imshow(annotated_image)\n",
    "plt.axis('off')  # Turn off axis labels\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffbd6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216644a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "input_dir = './Label-dataset/train'\n",
    "output_dir = './keypoint-dataset/train'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process each image in the directory\n",
    "for class_name in os.listdir(input_dir):\n",
    "    class_dir = os.path.join(input_dir, class_name)\n",
    "    if os.path.isdir(class_dir):\n",
    "        # Create the corresponding output class directory\n",
    "        output_class_dir = os.path.join(output_dir, class_name)\n",
    "        os.makedirs(output_class_dir, exist_ok=True)\n",
    "\n",
    "        for image_name in tqdm(os.listdir(class_dir), desc=f'Processing {class_name}'):\n",
    "            image_path = os.path.join(class_dir, image_name)\n",
    "            output_image_path = os.path.join(output_class_dir, image_name)\n",
    "\n",
    "            # Load the input image.\n",
    "            image = mp.Image.create_from_file(image_path)\n",
    "\n",
    "            # Detect pose landmarks from the input image.\n",
    "            detection_result = detector.detect(image)\n",
    "\n",
    "            # Process the detection result and draw landmarks on the image.\n",
    "            annotated_image = draw_landmarks_on_image(image.numpy_view(), detection_result)\n",
    "\n",
    "            # Save the annotated image\n",
    "            cv2.imwrite(output_image_path, cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05af3e44",
   "metadata": {},
   "source": [
    "### Buliding Simple CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d97e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(train_generator.num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=10,  # You can adjust the number of epochs\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34f716b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "ax[0].plot(history.history['accuracy'])\n",
    "ax[0].plot(history.history['val_accuracy'])\n",
    "ax[0].set_title('Model accuracy')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[0].legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "ax[1].plot(history.history['loss'])\n",
    "ax[1].plot(history.history['val_loss'])\n",
    "ax[1].set_title('Model loss')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457f1eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "accruracy,loss = model.evaluate(test_generator,verbose=0)\n",
    "print(\"CNN model accuracy:\",accruracy)\n",
    "print('CNN model Loss:',loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e3ef3a",
   "metadata": {},
   "source": [
    "### Building MobilenetV2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536ef90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MobileNetV2 model with pre-trained weights, excluding the top layers\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the base model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Create a new model on top\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(train_generator.num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=10,  # You can adjust the number of epochs\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850dff6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "ax[0].plot(history.history['accuracy'])\n",
    "ax[0].plot(history.history['val_accuracy'])\n",
    "ax[0].set_title('Model accuracy')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[0].legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "ax[1].plot(history.history['loss'])\n",
    "ax[1].plot(history.history['val_loss'])\n",
    "ax[1].set_title('Model loss')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e2ce6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accruracy,loss = model.evaluate(test_generator,verbose=0)\n",
    "print(\"MobileNetV2 model accuracy:\",accruracy)\n",
    "print('MobileNetV2 model Loss:',loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715b2e1e",
   "metadata": {},
   "source": [
    "### Combining Image dataset and Alphapose dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80614c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# Initialize Mediapipe PoseLandmarker\n",
    "base_options = mp.tasks.BaseOptions(model_asset_path='pose_landmarker_heavy.task')\n",
    "options = vision.PoseLandmarkerOptions(\n",
    "    base_options=base_options,\n",
    "    output_segmentation_masks=True)\n",
    "detector = vision.PoseLandmarker.create_from_options(options)\n",
    "def extract_landmarks(image_path):\n",
    "    image = mp.Image.create_from_file(image_path)\n",
    "    detection_result = detector.detect(image)\n",
    "    \n",
    "    landmarks_array = []\n",
    "    if detection_result.pose_landmarks:\n",
    "        for pose_landmarks in detection_result.pose_landmarks:\n",
    "            for landmark in pose_landmarks:\n",
    "                landmarks_array.extend([landmark.x, landmark.y, landmark.z])\n",
    "    else:\n",
    "        landmarks_array = [0] * 99  # Handle case when no landmarks are detected (33 keypoints * 3 coordinates)\n",
    "\n",
    "    return np.array(landmarks_array)\n",
    "\n",
    "def load_image_and_landmarks(file_path, label):\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, [224, 224])\n",
    "    img = img / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "    # Assuming landmarks are not part of the image file; replace this with actual landmark loading if needed\n",
    "    landmark = tf.zeros([99], dtype=tf.float32)  # Placeholder for landmarks\n",
    "    \n",
    "    return img, landmark, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5b3ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def load_data(image_dir):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "\n",
    "    image_dir = Path(image_dir)\n",
    "    \n",
    "    for folder in image_dir.iterdir():\n",
    "        if folder.is_dir():\n",
    "            for file in folder.iterdir():\n",
    "                if file.suffix == \".jpg\":\n",
    "                    image_paths.append(str(file))\n",
    "                    labels.append(folder.name)  # Use folder name as label\n",
    "\n",
    "    return image_paths, labels\n",
    "\n",
    "def process_path(file_path, label):\n",
    "    img, landmark, label = tf.py_function(load_image_and_landmarks, [file_path, label], [tf.float32, tf.float32, tf.float32])\n",
    "    img.set_shape((224, 224, 3))\n",
    "    landmark.set_shape((99,))\n",
    "    label.set_shape((num_classes,))  # Ensure the label shape matches the one-hot encoding\n",
    "    return (img, landmark), label\n",
    "\n",
    "def prepare_dataset(image_paths, labels, batch_size=32, buffer_size=1000):\n",
    "    global num_classes  # Declare num_classes as global to access it in process_path\n",
    "    unique_labels = np.unique(labels)\n",
    "    label_to_index = {label: index for index, label in enumerate(unique_labels)}\n",
    "    labels = np.array([label_to_index[label] for label in labels])\n",
    "    labels = tf.keras.utils.to_categorical(labels, num_classes=len(unique_labels))\n",
    "    \n",
    "    num_classes = len(unique_labels)  # Define num_classes here\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "    dataset = dataset.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.map(lambda x, y: ((x[0], x[1]), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(buffer_size).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset, num_classes\n",
    "\n",
    "# Load image paths and labels\n",
    "image_dir = './Label-dataset/train/'\n",
    "image_paths, labels = load_data(image_dir)\n",
    "\n",
    "# Prepare the dataset\n",
    "batch_size = 32\n",
    "dataset, num_classes = prepare_dataset(image_paths, labels, batch_size)\n",
    "def split_dataset(dataset, split_ratios=(0.7, 0.2, 0.1)):\n",
    "    # Calculate the sizes for each split\n",
    "    dataset_size = sum(1 for _ in dataset)\n",
    "    train_size = int(split_ratios[0] * dataset_size)\n",
    "    val_size = int(split_ratios[1] * dataset_size)\n",
    "    \n",
    "    # Shuffle dataset\n",
    "    dataset = dataset.shuffle(buffer_size=dataset_size, reshuffle_each_iteration=False)\n",
    "\n",
    "    # Split dataset\n",
    "    train_dataset = dataset.take(train_size)\n",
    "    val_dataset = dataset.skip(train_size).take(val_size)\n",
    "    test_dataset = dataset.skip(train_size + val_size)\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "split_ratios = (0.7, 0.2, 0.1)  # 70% training, 20% validation, 10% test\n",
    "train_dataset, val_dataset, test_dataset = split_dataset(dataset, split_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599476f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(image_dir):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "\n",
    "    image_dir = Path(image_dir)\n",
    "    \n",
    "    for folder in image_dir.iterdir():\n",
    "        if folder.is_dir():\n",
    "            for file in folder.iterdir():\n",
    "                if file.suffix == \".jpg\":\n",
    "                    image_paths.append(str(file))\n",
    "                    labels.append(folder.name)  # Use folder name as label\n",
    "\n",
    "    return image_paths, labels\n",
    "def load_image_and_landmarks(file_path, label):\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, [224, 224])\n",
    "    img = img / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "    # Assuming landmarks are not part of the image file; replace this with actual landmark loading if needed\n",
    "    landmark = tf.zeros([99], dtype=tf.float32)  # Placeholder for landmarks\n",
    "    \n",
    "    return img, landmark, label\n",
    "for file_path, label in zip(image_paths[:5], labels[:5]):  # Check first 5 samples\n",
    "    img, landmark, label = load_image_and_landmarks(file_path, label)\n",
    "    print(\"Image shape:\", img.shape)\n",
    "    print(\"Landmark shape:\", landmark.shape)\n",
    "    print(\"Label:\", label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02144bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (img, landmark), label in train_dataset.take(1):\n",
    "    print(\"Image shape:\", img.shape)\n",
    "    print(\"Landmark shape:\", landmark.shape)\n",
    "    print(\"Label shape:\", label.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89c6115",
   "metadata": {},
   "source": [
    "### Custom CNN model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2f6bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Concatenate, Conv2D, MaxPooling2D, Dropout\n",
    "\n",
    "# Image input branch\n",
    "image_input = Input(shape=(224, 224, 3), name='image_input')\n",
    "x = Conv2D(32, (3, 3), activation='relu')(image_input)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "\n",
    "# Landmarks input branch\n",
    "landmarks_input = Input(shape=(99,), name='landmarks_input')\n",
    "y = Dense(128, activation='relu')(landmarks_input)\n",
    "y = Dense(64, activation='relu')(y)\n",
    "\n",
    "# Combine branches\n",
    "combined = Concatenate()([x, y])\n",
    "z = Dense(512, activation='relu')(combined)\n",
    "z = Dense(256, activation='relu')(combined)\n",
    "z = Dense(num_classes, activation='softmax')(z)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=[image_input, landmarks_input], outputs=z)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d0a449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=20,\n",
    "    validation_data=val_dataset  # Replace with a validation dataset if available\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed058b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "ax[0].plot(history['accuracy'])\n",
    "ax[0].plot(history['val_accuracy'])\n",
    "ax[0].set_title('Model accuracy')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[0].legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "ax[1].plot(history['loss'])\n",
    "ax[1].plot(history['val_loss'])\n",
    "ax[1].set_title('Model loss')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c33a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "accruracy,loss = model.evaluate(test_dataset,verbose=0)\n",
    "print(\"CNN model with alphapose accuracy:\",accruracy)\n",
    "print('CNN model with alphapose Loss:',loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaa2701",
   "metadata": {},
   "source": [
    "### Proposed Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb83bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Concatenate, Dropout\n",
    "\n",
    "# Define MobileNetV2 as a feature extractor\n",
    "def build_model(num_classes):\n",
    "    # Image input branch with MobileNetV2\n",
    "    image_input = Input(shape=(224, 224, 3), name='image_input')\n",
    "    base_model = MobileNetV2(include_top=False, weights='imagenet', input_tensor=image_input, pooling='avg')\n",
    "    x = base_model.output\n",
    "    x = Flatten()(x)  # Flatten output of MobileNetV2\n",
    "\n",
    "    # Landmarks input branch\n",
    "    landmarks_input = Input(shape=(99,), name='landmarks_input')\n",
    "    y = Dense(128, activation='relu')(landmarks_input)\n",
    "    y = Dense(64, activation='relu')(y)\n",
    "\n",
    "    # Combine branches\n",
    "    combined = Concatenate()([x, y])\n",
    "    z = Dense(512, activation='relu')(combined)\n",
    "    z = Dropout(0.5)(z)\n",
    "    z = Dense(256, activation='relu')(z)\n",
    "    z = Dense(num_classes, activation='softmax')(z)\n",
    "\n",
    "    # Define the model\n",
    "    model = Model(inputs=[image_input, landmarks_input], outputs=z)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create the model with the number of classes\n",
    "model = build_model(num_classes)\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b32a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=20,\n",
    "    validation_data=train_dataset  # Replace with a validation dataset if available\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd56637b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "ax[0].plot(history['accuracy'])\n",
    "ax[0].plot(history['val_accuracy'])\n",
    "ax[0].set_title('Model accuracy')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[0].legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "ax[1].plot(history['loss'])\n",
    "ax[1].plot(history['val_loss'])\n",
    "ax[1].set_title('Model loss')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b92f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "accruracy,loss = model.evaluate(test_generator,verbose=0)\n",
    "print(\"MobileNetV2 Model with Alphapose accuracy:\",accruracy)\n",
    "print('MobileNetV2 Model with Alphapose Loss:',loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6744db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data\n",
    "models = ['CNN', 'MobileNetV2', 'CNN with Alphapose', 'MobileNetV2 with Alphapose']\n",
    "accuracy = [0.5324353, 0.7524353, 0.83297, 0.976530]\n",
    "\n",
    "# Plotting Accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(models, accuracy, color='b')\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy of Different Models')\n",
    "plt.ylim(0, 1)  # Accuracy typically ranges from 0 to 1\n",
    "\n",
    "# Adding value labels\n",
    "for i, v in enumerate(accuracy):\n",
    "    plt.text(i, v + 0.02, f'{v:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b360763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data\n",
    "models = ['CNN', 'MobileNetV2', 'CNN with Alphapose', 'MobileNetV2 with Alphapose']\n",
    "loss = [2.334243, 1.3829, 0.942732, 0.32245]\n",
    "\n",
    "# Plotting Loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(models, loss, color='r')\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss of Different Models')\n",
    "plt.ylim(0, max(loss) + 1)  # Set limit a bit higher than the max loss value\n",
    "\n",
    "# Adding value labels\n",
    "for i, v in enumerate(loss):\n",
    "    plt.text(i, v + 0.1, f'{v:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f16f6b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
